<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Hi, Jon Here. Please DELETE the two <script> tags below if you use this HTML, otherwise my analytics will track your page -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-7580334-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-7580334-2');
  </script>

  <title>Rooholla Khorrambakht</title>

  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Rooholla Khorrambakht</name>
              </p>
              <p>Currently, I am a research assistant at <a href="https://aras.kntu.ac.ir/">ARAS Labs</a>, where I work on percpetion and multi-modal state estimation for cable and surgical robots.
              </p>
              <p style="text-align:center">
                <a href="mailto:r.khorrambakht@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/r-khorrambakht/">LinkedIn</a>
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm interested in robotics control and perception. My research path aims towards designing algorithms and objectives that build upon physics and machine learning to enable safe and robust perception and control both as independent entities and combined wholes.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr onmouseout="nlt_stop()" onmouseover="nlt_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/uvo.png' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
      			  <br>
                    <a href="https://https://arxiv.org/abs/2107.00366">
                      <papertitle>A Consistency-Based Loss for Deep Odometry Through Uncertainty Propagation</papertitle>
                    </a>
      			  <br>
              <a href="http://hamed-d.github.io">Hamed Damirchi</a>,
              <strong>Rooholla Khorrambakht</strong>,
              <a href="https://aras.kntu.ac.ir/taghirad/">Hamid D. Taghirad</a>
              <a href="https://scholar.google.ca/citations?user=SmL7-6YAAAAJ&hl=en">Behzad Moshiri</a>
              <br>
              <br>
              <a href="https://https://arxiv.org/abs/2107.00366">arXiv</a>
              <p></p>
              <p>We exploit the Lie theory to define PDFs over SE(3) manifolds. Then, A maximum likelihood formulation is adopted to
                teach the model about the aleatoric uncertainty. Finally, a balance between the long and short-term odometry losses has been
                defined through the propagation of this uncertainty.</p>
            </td>
          </tr>
      		  <tr onmouseout="nerfie_stop()" onmouseover="nerfie_start()">
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                      <div class="two" id='nerfie_image'><video  width=160 muted autoplay loop>
                      <source src="data/avo-dynamic.mp4" type="video/mp4">
                      Your browser does not support the video tag.
                      </video></div>
                      <img src='data/avo_still.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function nerfie_start() {
                        document.getElementById('nerfie_image').style.opacity = "1";
                      }

                      function nerfie_stop() {
                        document.getElementById('nerfie_image').style.opacity = "0";
                      }
                      nerfie_stop()
                    </script>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://nerfies.github.io/">
                      <papertitle>Exploring Self-Attention for Visual Odometry</papertitle>
                    </a>
                    <br>

                    <a href="http://hamed-d.github.io">Hamed Damirchi</a>,
                    <strong>Rooholla Khorrambakht</strong>,
                    <a href="https://aras.kntu.ac.ir/taghirad/">Hamid D. Taghirad</a>
                    <br>
                    <em>arXiv</em>, 2020
                    <br>
                    <a href="https://arxiv.org/pdf/2011.08634v1.pdf">arXiv</a>
                    <p></p>
                    <p>We show that common CNN-LSTM architectures fail to extract adequate motion features in the presense of artifacts in the input. Then, by using a single-head spatial self-attention mechanism and without augmentations, we show that the network is able to reject dynamic objects and focus on the background of the scene for better odometry.
                    </p>
                  </td>
                </tr>

                <tr onmouseout="nlt_stop()" onmouseover="nlt_start()">
                  <td style="padding:20px;width:25%;vertical-align:middle">
                      <img src='images/capsnet.png' width="160">
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
      			  <br>
                    <a href="https://arxiv.org/abs/2007.03063">
                      <papertitle>ARC-Net: Activity Recognition Through Capsules</papertitle>
                    </a>
      			  <br>
                    <a href="http://hamed-d.github.io">Hamed Damirchi</a>,
                    <strong>Rooholla Khorrambakht</strong>,
                    <a href="https://aras.kntu.ac.ir/taghirad/">Hamid D. Taghirad</a>
                    <br>
                    <a href="https://www.icmla-conference.org/icmla20/index.html"><em>ICMLA</em>, 2020</a>
                    <br>
                    <a href="https://arxiv.org/abs/2007.03063">arXiv</a>
                    <p></p>
                    <p>A single CNN encoder is used to extract the features from multiple IMU sensors. These features are used as entities that represent patterns in the input and through dynamic routing, activity of the subject is derived in that window of inputs. By using this architecture, various benefits such as interpretability and robustnetss to noise are gained.</p>
                  </td>
                </tr>

                <tr onmouseout="nerfw_stop()" onmouseover="nerfw_start()">
                  <td style="padding:20px;width:25%;vertical-align:middle">
                      <img src='images/preintegration.png' width="160">
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://arxiv.org/pdf/2007.02929.pdf">
                      <papertitle>Preintegrated IMU Features For Efficient Deep Inertial Odometry</papertitle>
                    </a>
                    <br>
                    <strong>Rooholla Khorrambakht</strong>,
                    <a href="http://hamed-d.github.io">Hamed Damirchi</a>,
                    <a href="https://aras.kntu.ac.ir/taghirad/">Hamid D. Taghirad</a>
                    <br>
                    <em>arXiv</em>, 2020
                    <br>
                    <a href="https://arxiv.org/pdf/2007.02929.pdf">arXiv</a>
                    <p></p>
                    <p>A computationally efficient inertial representation for deep inertial odometry is proposed by replacing the raw IMU data in deep Inertial models with preintegrated features to improve the modelâ€™s efficiency.</p>
                  </td>
                </tr>

                <tr onmouseout="ff_stop()" onmouseover="ff_start()">
                  <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/arasiref.png' width="160">
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
      		      <br>
                    <a href="https://ieeexplore.ieee.org/document/9071852">
                      <papertitle>ARAS-IREF: An Open-Source Low-Cost Framework for Pose Estimation</papertitle>
                    </a>
                    <br>
                    <a href="http://hamed-d.github.io">Hamed Damirchi</a>,
                    <strong>Rooholla Khorrambakht</strong>,
                    <a href="https://aras.kntu.ac.ir/taghirad/">Hamid D. Taghirad</a>
                    <br>
                    <em>ICROM</em>, 2019
                    <br>
                    <a href="https://ieeexplore.ieee.org/document/9071852">IEEE</a>
                    <p></p>
                    <p>In order to be able to collect necessary data for data driven algorithms, we needed a pose estimation framework that would be accurate while providing ground truth at a fairly high frequency. ARAS-IREF is composed of a camera alongside a linux enabled dev board that allows for pose estimation at 100+Hz.</p>
                  </td>
                </tr>


                <tr onmouseout="ff_stop()" onmouseover="ff_start()">
                  <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/calib.png' width="160">
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
      		      <br>
                    <a href="https://ieeexplore.ieee.org/document/9071903">
                      <papertitle>A Calibration Framework for Deployable Cable Driven Parallel Robots with Flexible Cables</papertitle>
                    </a>
                    <br>
                    <strong>Rooholla Khorrambakht</strong>,
                    <a href="http://hamed-d.github.io">Hamed Damirchi</a>,
                    <a href="https://aras.kntu.ac.ir/taghirad/">Hamid D. Taghirad</a>
                    <br>
                    <em>ICROM</em>, 2019
                    <br>
                    <a href="https://ieeexplore.ieee.org/document/9071903">IEEE</a>
                    <p></p>
                    <p>An effective framework for calibrating the kinematic parameters of suspended cable driven parallel robots with no requirements for expensive tools and measurement devices is proposed where this algorithm utilizes the existing force sensors in the cable robot to nominate the best set of data for calibration.</p>
                  </td>
                </tr>

        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                The template is from <a href="https://jonbarron.info/">John Barron</a>'s <a href="https://github.com/jonbarron/website">rep</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
